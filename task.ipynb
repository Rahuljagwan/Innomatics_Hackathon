{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EjENhySDvdWW",
        "outputId": "e2bff24c-4913-4e74-ed64-10206831ab23"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ba089818-8023-4c93-a8e6-e70f71babf88\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ba089818-8023-4c93-a8e6-e70f71babf88\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Ramamurthy Nagar.csv to Ramamurthy Nagar.csv\n",
            "Saving Banashankari.csv to Banashankari.csv\n",
            "Saving Bellandur.csv to Bellandur.csv\n",
            "Saving Bilekahalli.csv to Bilekahalli.csv\n",
            "Saving BTM 2nd Stage.csv to BTM 2nd Stage.csv\n",
            "Saving Chikkalasandra.csv to Chikkalasandra.csv\n",
            "Saving Electronic City.csv to Electronic City.csv\n",
            "Saving Gottigere.csv to Gottigere.csv\n",
            "Saving Hebbal.csv to Hebbal.csv\n",
            "Saving Hulimavu.csv to Hulimavu.csv\n",
            "Saving Indiranagar.csv to Indiranagar.csv\n",
            "Saving Jayanagar.csv to Jayanagar.csv\n",
            "Saving JP Nagar.csv to JP Nagar.csv\n",
            "Saving Krishnarajapura.csv to Krishnarajapura.csv\n",
            "Saving Kumaraswamy Layout.csv to Kumaraswamy Layout.csv\n",
            "Saving Mahadevapura.csv to Mahadevapura.csv\n",
            "Saving Marathahalli.csv to Marathahalli.csv\n",
            "Saving Nagarbhavi.csv to Nagarbhavi.csv\n",
            "Saving Rajaji Nagar.csv to Rajaji Nagar.csv\n",
            "Saving RR Nagar.csv to RR Nagar.csv\n",
            "Saving RT Nagar.csv to RT Nagar.csv\n",
            "Saving Vijaya Nagar.csv to Vijaya Nagar.csv\n",
            "Saving Whitefield.csv to Whitefield.csv\n",
            "Saving Yelahanka.csv to Yelahanka.csv\n",
            "Saving Akshayanagar.csv to Akshayanagar.csv\n",
            "Saving Arekere.csv to Arekere.csv\n",
            "Saving Basavanagudi.csv to Basavanagudi.csv\n",
            "Saving Basaveshwar Nagar.csv to Basaveshwar Nagar.csv\n",
            "Saving Begur.csv to Begur.csv\n",
            "Saving Doddanekundi.csv to Doddanekundi.csv\n",
            "Saving Ejipura.csv to Ejipura.csv\n",
            "Saving HBR Layout.csv to HBR Layout.csv\n",
            "Saving Hongasandra.csv to Hongasandra.csv\n",
            "Saving Hoodi.csv to Hoodi.csv\n",
            "Saving Horamavu.csv to Horamavu.csv\n",
            "Saving K.R Puram.csv to K.R Puram.csv\n",
            "Saving Kaggadasapura.csv to Kaggadasapura.csv\n",
            "Saving Kammanahalli.csv to Kammanahalli.csv\n",
            "Saving Kengeri.csv to Kengeri.csv\n",
            "Saving Lingarajapuram.csv to Lingarajapuram.csv\n",
            "Saving Munnekollal.csv to Munnekollal.csv\n",
            "Saving Padmanabhanagar.csv to Padmanabhanagar.csv\n",
            "Saving Singasandra.csv to Singasandra.csv\n",
            "Saving Thanisandra.csv to Thanisandra.csv\n",
            "Saving Uttarahalli Hobli.csv to Uttarahalli Hobli.csv\n",
            "Saving Varthur.csv to Varthur.csv\n",
            "Saving Vidyaranyapura.csv to Vidyaranyapura.csv\n",
            "Saving Bommanahalli.csv to Bommanahalli.csv\n",
            "Saving Brookefield.csv to Brookefield.csv\n",
            "Saving BTM_Layout.csv to BTM_Layout.csv\n",
            "Saving Electronics City Phase 1.csv to Electronics City Phase 1.csv\n",
            "Saving Hosakerehalli.csv to Hosakerehalli.csv\n",
            "Saving HSR_Layout.csv to HSR_Layout.csv\n",
            "Saving Kadugodi.csv to Kadugodi.csv\n",
            "Saving Kalyan Nagar.csv to Kalyan Nagar.csv\n",
            "Saving Kasavanahalli.csv to Kasavanahalli.csv\n",
            "Saving Kengeri Satellite Town.csv to Kengeri Satellite Town.csv\n",
            "Saving Konanakunte.csv to Konanakunte.csv\n",
            "Saving Koramangala.csv to Koramangala.csv\n",
            "Saving Mathikere.csv to Mathikere.csv\n",
            "Saving Raja Rajeshwari Nagar.csv to Raja Rajeshwari Nagar.csv\n",
            "Saving Subramanyapura.csv to Subramanyapura.csv\n",
            "Saving Sunkadakatte.csv to Sunkadakatte.csv\n",
            "Saving Yelahanka New Town.csv to Yelahanka New Town.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will open a file picker to upload files.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "file_paths = glob.glob(\"*.csv\")  # This will list all CSV files in the current directory.\n"
      ],
      "metadata": {
        "id": "98Pac1xZxF-L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Load all CSV files\n",
        "file_paths = glob.glob(\"datasets/*.csv\")\n",
        "\n",
        "# Initialize an empty list to hold dataframes\n",
        "dataframes = []\n",
        "\n",
        "# Read each CSV and append it to the list\n",
        "for file_path in file_paths:\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, encoding='latin1')  # or 'iso-8859-1', 'cp1252'\n",
        "        dataframes.append(df)\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Error reading {file_path}. Trying a different encoding.\")\n",
        "        df = pd.read_csv(file_path, encoding='iso-8859-1')\n",
        "        dataframes.append(df)\n",
        "\n",
        "# Concatenate all dataframes\n",
        "final_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Perform data cleaning (examples shown below)\n",
        "final_df['activation_date'] = pd.to_datetime(final_df['activation_date'],dayfirst=True, errors='coerce')\n",
        "final_df['bathroom'] = final_df['bathroom'].fillna(0).astype(int)\n",
        "# Additional cleaning steps...\n",
        "\n",
        "# Save the cleaned and merged dataframe\n",
        "final_df.to_csv(\"final_property_data.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "nBvc2ErGxR4L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = glob.glob(\"datasets/*.csv\")\n",
        "print(file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCBPWolmxxMr",
        "outputId": "3ec41316-60ce-446e-c579-f2e25abba57e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['datasets/Bommanahalli.csv', 'datasets/Hongasandra.csv', 'datasets/Krishnarajapura.csv', 'datasets/Thanisandra.csv', 'datasets/Konanakunte.csv', 'datasets/Varthur.csv', 'datasets/Mathikere.csv', 'datasets/Nagarbhavi.csv', 'datasets/Vijaya Nagar.csv', 'datasets/Akshayanagar.csv', 'datasets/Hoodi.csv', 'datasets/Mahadevapura.csv', 'datasets/Yelahanka.csv', 'datasets/HBR Layout.csv', 'datasets/Electronics City Phase 1.csv', 'datasets/Kumaraswamy Layout.csv', 'datasets/Munnekollal.csv', 'datasets/Basavanagudi.csv', 'datasets/Padmanabhanagar.csv', 'datasets/Hosakerehalli.csv', 'datasets/K.R Puram.csv', 'datasets/Indiranagar.csv', 'datasets/Kengeri Satellite Town.csv', 'datasets/Gottigere.csv', 'datasets/Hulimavu.csv', 'datasets/Singasandra.csv', 'datasets/Rajaji Nagar.csv', 'datasets/Sunkadakatte.csv', 'datasets/Subramanyapura.csv', 'datasets/RT Nagar.csv', 'datasets/Bilekahalli.csv', 'datasets/Kadugodi.csv', 'datasets/Arekere.csv', 'datasets/Ejipura.csv', 'datasets/Jayanagar.csv', 'datasets/JP Nagar.csv', 'datasets/Uttarahalli Hobli.csv', 'datasets/Kengeri.csv', 'datasets/RR Nagar.csv', 'datasets/Horamavu.csv', 'datasets/Begur.csv', 'datasets/Kaggadasapura.csv', 'datasets/Yelahanka New Town.csv', 'datasets/Marathahalli.csv', 'datasets/Vidyaranyapura.csv', 'datasets/Bellandur.csv', 'datasets/Whitefield.csv', 'datasets/Koramangala.csv', 'datasets/BTM_Layout.csv', 'datasets/Ramamurthy Nagar.csv', 'datasets/Chikkalasandra.csv', 'datasets/Electronic City.csv', 'datasets/Lingarajapuram.csv', 'datasets/Kammanahalli.csv', 'datasets/Brookefield.csv', 'datasets/Raja Rajeshwari Nagar.csv', 'datasets/Basaveshwar Nagar.csv', 'datasets/Doddanekundi.csv', 'datasets/Kasavanahalli.csv', 'datasets/Kalyan Nagar.csv', 'datasets/BTM 2nd Stage.csv', 'datasets/Banashankari.csv', 'datasets/Hebbal.csv', 'datasets/HSR_Layout.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('final_property_data.csv')\n",
        "\n",
        "# Print the column names\n",
        "# print(df.columns)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "# Check if 'gym' column exists, then fill missing values\n",
        "if 'gym' in df.columns:\n",
        "    df['gym'] = df['gym'].fillna(0)\n",
        "\n",
        "# Check if 'lift' column exists, then fill missing values\n",
        "if 'lift' in df.columns:\n",
        "    df['lift'] = df['lift'].fillna(0)\n",
        "\n",
        "# Standardize date format\n",
        "if 'activation_date' in df.columns:\n",
        "    df['activation_date'] = pd.to_datetime(df['activation_date'])\n",
        "\n",
        "# Convert numeric columns, handle NaN values by filling before conversion\n",
        "if 'bathroom' in df.columns:\n",
        "    df['bathroom'] = df['bathroom'].fillna(0).astype(int)\n",
        "\n",
        "if 'floor' in df.columns:\n",
        "    df['floor'] = df['floor'].fillna(0).astype(int)\n",
        "\n",
        "# Handle corrupted JSON-like strings in photo_urls\n",
        "def clean_photo_urls(photo_url):\n",
        "    try:\n",
        "        photo_list = json.loads(photo_url.replace(\"'\", \"\\\"\"))\n",
        "        return len(photo_list)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# Check if 'photo_urls' exists\n",
        "if 'photo_urls' in df.columns:\n",
        "    df['photo_count'] = df['photo_urls'].apply(clean_photo_urls)\n",
        "\n",
        "# Remove duplicates based on 'property_id'\n",
        "if 'property_id' in df.columns:\n",
        "    df = df.drop_duplicates(subset='property_id')\n",
        "\n",
        "# Handle outliers by capping rent values\n",
        "if 'rent' in df.columns:\n",
        "    df['rent'] = df['rent'].clip(lower=500, upper=100000)\n",
        "\n",
        "# Standardize categorical values in 'type' column\n",
        "if 'type' in df.columns:\n",
        "    df['type'] = df['type'].str.lower()\n",
        "\n",
        "# Clean string columns (e.g., 'locality')\n",
        "if 'locality' in df.columns:\n",
        "    df['locality'] = df['locality'].str.strip().str.title()\n",
        "\n"
      ],
      "metadata": {
        "id": "AfYzIgXe3TR1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Step 1: Load the data from the TSV file\n",
        "df = pd.read_csv('property_photos.tsv', sep='\\t')\n",
        "\n",
        "# Step 2: Function to clean the 'photo_urls' and count the photos\n",
        "def count_photos(photo_urls):\n",
        "    if pd.isna(photo_urls) or photo_urls == 'NULL':\n",
        "        return 0  # If no photos, return 0\n",
        "\n",
        "    try:\n",
        "        # Remove any escape characters (like \\) that may be present due to the corrupted JSON\n",
        "        cleaned_photo_urls = re.sub(r'\\\\([\":,{}\\[\\]])', r'\\1', photo_urls)  # Clean the escaped characters\n",
        "\n",
        "        # Now, parse the JSON string\n",
        "        photos = json.loads(cleaned_photo_urls)\n",
        "\n",
        "        # Return the number of photos\n",
        "        return len(photos)\n",
        "    except json.JSONDecodeError:\n",
        "        # In case of a JSON decode error, return 0\n",
        "        return 0\n",
        "\n",
        "# Step 3: Apply the function to the 'photo_urls' column\n",
        "df['photo_count'] = df['photo_urls'].apply(count_photos)\n",
        "\n",
        "# Step 4: Verify the cleaned data\n",
        "print(df[['property_id', 'photo_count']].head())  # Display a sample of the cleaned data\n",
        "\n",
        "# Step 5: Save the cleaned dataset to a new file\n",
        "df.to_csv('cleaned_property_photos.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wd5GCUn97a0",
        "outputId": "c53f8326-9bd2-4f04-920e-520adf545e69"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        property_id  photo_count\n",
            "0  ff808081469fd6e20146a5af948000ea            0\n",
            "1  ff8080814702d3d10147068359d200cd            0\n",
            "2  ff808081470c645401470fb03f5800a6            0\n",
            "3  ff808081470c6454014715eaa5960281            0\n",
            "4  ff808081474aa867014771a0298f0aa6            0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the property photos data (adjust the path and filename as needed)\n",
        "property_photos = pd.read_csv('property_photos.tsv', sep='\\t')\n",
        "\n",
        "# Now check if the 'photo_urls' column exists\n",
        "print(property_photos.columns)  # List the column names\n",
        "\n",
        "\n",
        "# Check if the column exists in the dataframe\n",
        "if 'photo_urls' in property_photos.columns:\n",
        "    def count_photos(photo_urls):\n",
        "        if pd.isna(photo_urls):\n",
        "            return 0\n",
        "        try:\n",
        "            # Try to load the photo URLs as JSON\n",
        "            urls = json.loads(photo_urls)  # Assuming it's a list of URLs in a JSON string format\n",
        "            return len(urls)  # Count the number of photo URLs\n",
        "        except json.JSONDecodeError:\n",
        "            return 0  # Return 0 if the string is corrupted\n",
        "\n",
        "    # Apply the function to create the 'photo_count' column\n",
        "    property_photos['photo_count'] = property_photos['photo_urls'].apply(count_photos)\n",
        "\n",
        "else:\n",
        "    print(\"The 'photo_urls' column is missing or incorrectly named in final_df.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg3EfjGDFesT",
        "outputId": "5b091f71-293a-4a13-b700-30699f6b6c33"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['property_id', 'photo_urls'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the property photos data (adjust the path and filename as needed)\n",
        "property_photos = pd.read_csv('property_photos.tsv', sep='\\t')\n",
        "\n",
        "# Now check if the 'photo_urls' column exists\n",
        "print(property_photos.columns)  # List the column names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3O_L1FsHSf9",
        "outputId": "e14c3f1b-59d9-409f-bc67-052a483e792d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['property_id', 'photo_urls'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_df.columns)  # Check the column names of the final_df dataframe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Web3CI1GOw5",
        "outputId": "ba5b3ef1-e1b9-4410-d922-23977378efd5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['property_id', 'type', 'activation_date', 'bathroom', 'floor',\n",
            "       'total_floor', 'furnishing', 'gym', 'latitude', 'longitude',\n",
            "       'lease_type', 'lift', 'location', 'parking', 'property_age',\n",
            "       'property_size', 'swimming_pool', 'pin_code', 'rent', 'deposit',\n",
            "       'building_type', 'locality',\n",
            "       'property_id/type/activation_date/bathroom/floor/total_floor/furnishing/gym/latitude/longitude/lease_type/lift/locality/parking/property_age/property_size/swimming_pool/pin_code/rent/deposit/building_type',\n",
            "       'ÿþ'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "final_df = pd.read_csv('final_property_data.csv')\n",
        "\n",
        "# 6. Final shape of the combined property data after merging all location CSV files.\n",
        "print(final_df.shape)  # Output will be in the format (rows, columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVAuql9_GYlJ",
        "outputId": "9dd40738-1f70-4ae5-ee93-7a3b7fda42d5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14532, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Percentage of properties located in HSR Layout\n",
        "hsr_count = final_df[final_df['locality'] == 'HSR Layout'].shape[0]\n",
        "hsr_percentage = (hsr_count / final_df.shape[0]) * 100\n",
        "print(round(hsr_percentage))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TqxIvz7I6UV",
        "outputId": "e285485f-f839-43d4-d73c-2a0aaee11786"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Number of unique locations\n",
        "unique_locations = final_df['locality'].nunique()\n",
        "print(unique_locations)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3RZhYfJI-2a",
        "outputId": "eb625f33-a8c8-416e-80c6-0d3b2e3ad533"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Number of unique locations\n",
        "unique_locations = final_df['locality'].nunique()\n",
        "print(unique_locations)\n",
        "\n",
        "# 9. Locality with the highest average rent\n",
        "avg_rent_by_locality = final_df.groupby('locality')['rent'].mean()\n",
        "highest_rent_locality = avg_rent_by_locality.idxmax()\n",
        "print(highest_rent_locality)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__gxjNTSJKF8",
        "outputId": "0aa05fbb-2b98-49f1-9a36-3515401d898e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61\n",
            "Bellandur\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Feature with the highest correlation with rent\n",
        "\n",
        "# Select only numeric columns from final_df\n",
        "numeric_columns = final_df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Compute correlations only for numeric columns\n",
        "correlation = numeric_columns.corr()\n",
        "\n",
        "# Get the feature with the highest correlation with 'rent'\n",
        "highest_corr_feature = correlation['rent'].idxmax()\n",
        "\n",
        "print(f\"The feature with the highest correlation with rent is: {highest_corr_feature}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJlH0MkUJTzp",
        "outputId": "4153df5c-704c-40ee-b681-4e195327e3ba"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The feature with the highest correlation with rent is: rent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'activation_date' is in datetime format\n",
        "final_df['activation_date'] = pd.to_datetime(final_df['activation_date'], errors='coerce')\n",
        "\n",
        "# Check if there is a column 'property_id' and 'activation_date'\n",
        "if 'property_id' in final_df.columns and 'activation_date' in final_df.columns:\n",
        "    # Calculate interaction count for each property\n",
        "    final_df['interaction_count'] = final_df.groupby('property_id')['property_id'].transform('size')\n",
        "\n",
        "    # Calculate days since activation\n",
        "    final_df['days_since_activation'] = (pd.to_datetime('today') - final_df['activation_date']).dt.days\n",
        "\n",
        "    # Filter properties within 7 days of activation\n",
        "    recent_interactions = final_df[final_df['days_since_activation'] <= 7]\n",
        "\n",
        "    # Check if there are any properties in the filtered data\n",
        "    if not recent_interactions.empty:\n",
        "        majority_interactions = recent_interactions['interaction_count'].mode()\n",
        "\n",
        "        # Ensure that mode() has a valid result\n",
        "        if not majority_interactions.empty:\n",
        "            print(f\"Majority of properties within 7 days of activation have {majority_interactions.iloc[0]} interactions.\")\n",
        "        else:\n",
        "            print(\"No mode found for interactions within 7 days.\")\n",
        "    else:\n",
        "        print(\"No properties activated within the last 7 days.\")\n",
        "else:\n",
        "    print(\"Missing 'property_id' or 'activation_date' columns in the dataset.\")\n",
        "\n",
        "\n",
        "# 12. Create property_age_category feature and identify the most frequent category\n",
        "def categorize_age(age):\n",
        "    if age <= 1:\n",
        "        return 'New'\n",
        "    elif 1 < age <= 5:\n",
        "        return 'Less than 5 years'\n",
        "    elif 5 < age <= 10:\n",
        "        return '5 to 10 years'\n",
        "    elif 10 < age <= 20:\n",
        "        return '10 to 20 years'\n",
        "    else:\n",
        "        return 'More than 20 years'\n",
        "\n",
        "final_df['property_age_category'] = final_df['property_age'].apply(categorize_age)\n",
        "most_frequent_category = final_df['property_age_category'].mode()[0]\n",
        "print(most_frequent_category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjeaERutJfrg",
        "outputId": "443897c7-cb18-4e4a-a944-a36eefc34744"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No properties activated within the last 7 days.\n",
            "New\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Apartment type with the highest average interactions\n",
        "# Assuming that type refers to BHK categories in the 'type' column\n",
        "avg_interactions_by_type = final_df.groupby('type')['interaction_count'].mean()\n",
        "highest_avg_interactions_type = avg_interactions_by_type.idxmax()\n",
        "print(highest_avg_interactions_type)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PMpcr0kJxvH",
        "outputId": "a6bbd3c8-4669-4829-d5d5-de7c8422e2e0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BHK1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Amenity with the greatest impact on rent\n",
        "# Let's check the correlation of amenities with rent (gym, lift, swimming_pool)\n",
        "amenities = ['gym', 'lift', 'swimming_pool']\n",
        "\n",
        "# Ensure columns are numeric and handle any missing values\n",
        "for amenity in amenities:\n",
        "    if amenity in final_df.columns:\n",
        "        # Convert the column to numeric, replacing non-numeric values with NaN (errors='coerce')\n",
        "        final_df[amenity] = pd.to_numeric(final_df[amenity], errors='coerce')\n",
        "\n",
        "        # Fill any missing values with 0 (assuming absence of amenity is represented by 0)\n",
        "        final_df[amenity] = final_df[amenity].fillna(0).astype(int)\n",
        "\n",
        "        # Calculate and print correlation with rent\n",
        "        correlation_value = final_df[amenity].corr(final_df['rent'])\n",
        "        print(f\"Correlation of {amenity} with rent: {correlation_value:.4f}\")\n",
        "    else:\n",
        "        print(f\"Column '{amenity}' is missing in the dataframe.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiqe1VAIKl5B",
        "outputId": "2b01b372-ffb4-4b59-c586-49ec313f67a4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation of gym with rent: 0.3896\n",
            "Correlation of lift with rent: 0.4085\n",
            "Correlation of swimming_pool with rent: 0.3916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Highest interaction counts for properties in the top 5 localities with the highest average rent\n",
        "top_localities = avg_rent_by_locality.nlargest(5).index\n",
        "top_localities_data = final_df[final_df['locality'].isin(top_localities)]\n",
        "highest_interactions_localities = top_localities_data.groupby('locality')['interaction_count'].max()\n",
        "print(highest_interactions_localities)\n",
        "\n",
        "# 16. Photo count of the property that received the highest number of total interactions\n",
        "\n",
        "# Check if 'interaction_count' and 'photo_count' columns exist in the dataframe\n",
        "if 'interaction_count' in final_df.columns and 'photo_count' in final_df.columns:\n",
        "    # Ensure 'interaction_count' has no NaN values before applying idxmax\n",
        "    final_df = final_df.dropna(subset=['interaction_count'])\n",
        "\n",
        "    # Get the property with the highest interaction count\n",
        "    highest_interaction_property = final_df.loc[final_df['interaction_count'].idxmax()]\n",
        "\n",
        "    # Get the photo count of the property with the highest interaction count\n",
        "    photo_count_highest_interaction = highest_interaction_property['photo_count']\n",
        "    print(f\"Photo count of the property with the highest interaction count: {photo_count_highest_interaction}\")\n",
        "else:\n",
        "    print(\"Columns 'interaction_count' or 'photo_count' are missing in the dataframe.\")\n",
        "\n",
        "\n",
        "# 17. Hypothesis test to compare the average interactions of properties with and without a gym\n",
        "with_gym = final_df[final_df['gym'] == 1]['interaction_count']\n",
        "without_gym = final_df[final_df['gym'] == 0]['interaction_count']\n",
        "\n",
        "# Perform independent t-test\n",
        "t_stat, p_value = stats.ttest_ind(with_gym, without_gym)\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "else:\n",
        "    print(\"Failed to reject the null hypothesis\")\n",
        "\n",
        "# 18. Create 'time_category' based on activation hour\n",
        "final_df['activation_hour'] = final_df['activation_date'].dt.hour\n",
        "\n",
        "def categorize_time(hour):\n",
        "    if 0 <= hour < 6:\n",
        "        return 'Midnight'\n",
        "    elif 6 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 18:\n",
        "        return 'Afternoon'\n",
        "    else:\n",
        "        return 'Evening'\n",
        "\n",
        "final_df['time_category'] = final_df['activation_hour'].apply(categorize_time)\n",
        "most_frequent_time_category = final_df['time_category'].mode()[0]\n",
        "print(most_frequent_time_category)\n",
        "\n",
        "# 19. Date with the most properties activated\n",
        "activation_counts = final_df['activation_date'].dt.date.value_counts()\n",
        "most_activated_date = activation_counts.idxmax()\n",
        "print(most_activated_date)\n",
        "\n",
        "# 20. Percentage of properties available for lease under the 'Anyone' category\n",
        "anyone_lease_count = final_df[final_df['lease_type'] == 'Anyone'].shape[0]\n",
        "anyone_lease_percentage = (anyone_lease_count / final_df.shape[0]) * 100\n",
        "print(round(anyone_lease_percentage))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzWfShhYKv7T",
        "outputId": "974cbe50-9da6-4ff7-8db7-87a576f1b7b5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "locality\n",
            "Basavanagudi     1.0\n",
            "Bellandur        1.0\n",
            "Brookefield      1.0\n",
            "Kasavanahalli    1.0\n",
            "Whitefield       1.0\n",
            "Name: interaction_count, dtype: float64\n",
            "Columns 'interaction_count' or 'photo_count' are missing in the dataframe.\n",
            "Failed to reject the null hypothesis\n",
            "Afternoon\n",
            "2017-03-17\n",
            "0\n"
          ]
        }
      ]
    }
  ]
}